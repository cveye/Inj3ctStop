# Inj3ctStop â€” Next-Gen LLM Safety Stack

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-live-brightgreen.svg)](#)
[![Tests](https://img.shields.io/badge/tests-passing-success.svg)](#)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-blueviolet.svg)](https://github.com/cveye/Inj3ctStop/pulls)
[![Made with â¤ï¸](https://img.shields.io/badge/made%20with-%E2%9D%A4-red.svg)](#)
[![GitHub stars](https://img.shields.io/github/stars/cveye/Inj3ctStop?style=social)](https://github.com/cveye/Inj3ctStop/stargazers)
[![Last Commit](https://img.shields.io/github/last-commit/cveye/Inj3ctStop/main)](https://github.com/cveye/Inj3ctStop/commits/main)

---

**Stop prompt injections. Ship safer LLM apps.**

Inj3ctStop is a lightweight, defensive playground for experimenting with **prompt injection attacks** and **layered mitigation patterns**.  
The goal: build a **â€œnext-gen LLM safety stackâ€** through iterative experiments.

All experiments use **simulated inputs** for learning and research. This repo is educational and defensive only â€” it does **not** contain real jailbreak payloads.

---

## ğŸ“š Experiments

[![What's New](https://img.shields.io/badge/What's%20New-v3%20--%20Context--Aware%20Detection-brightgreen.svg)](https://github.com/cveye/Inj3ctStop/tree/main/experiments/v3-context-aware)

### ğŸ”¹ v1 â€” Simple Filters & Scoped Prompts
- Shows why naive wrappers and regex sanitization fail ğŸš¨
- Demonstrates how **scoped prompts + verification** perform better âœ…
- Uses **DistilGPT2** as a lightweight LLM model
- [See code](experiments/v1-simple-filters)

### ğŸ”¹ v2 â€” Chained Guards Pipeline
- Introduces a **multi-layer defense stack**
- Layers include:
  1. Regex prefilter (block obvious bad inputs)
  2. Semantic embedding check (MiniLM)
  3. LLM self-check (DistilGPT2, perplexity/keywords)
  4. Sandbox post-process (redacts sensitive values)
- Provides:
  - Flask web demo (`/demo`)
  - Benchmark runner with `attack_corpus.json`
  - Unit tests per guard and end-to-end
- [See code](experiments/v2-chained-guards)

### v3 â€” Context-Aware Injection Detection (Memory Makes Prompt Injections Dangerous)

- Adds short-term session memory (last N turns) and a context guard that compares the new prompt against recent history using MiniLM embeddings.
- Flags two classes:
    - `explicit_override` â†’ phrases like â€œignore previousâ€, â€œforget earlierâ€, â€œreset rulesâ€.
    - `context_shift` â†’ sudden semantic drift from the recent user turn (low similarity).
- Memory toggle in the demo UI so you can see how outcomes change with/without context.

- [See code](experiments/v3-context-aware)

More experiments are planned â€” each adds realism and complexity to the stack.

---

## ğŸš€ Quickstart

Clone and run an experiment:

```bash
git clone https://github.com/cveye/Inj3ctStop.git
cd Inj3ctStop/experiments/v2-chained-guards

make install
make run
```
## ğŸ§ª Benchmarks
Each experiment includes a runner that executes a corpus of benign and malicious prompts.

Example for v2:

```bash
cd experiments/v3-context-aware
make install
make run

# then open http://localhost:8080/v3exp
```
![alt text](resources/memory_context.jpg)

## ğŸ“Š Roadmap
- v1: âœ… Simple filters + scoped prompts

- v2: âœ… Chained guards pipeline

- v3: âœ… Context-aware detection (memory + context guard)
- v4: ğŸ”„ Signature + heuristic engine (IDS-style rules + entropy/keyword scores)

- Vn: ğŸ”„ More to come ....

## ğŸ¤ Contributing
Contributions, ideas, issues, and PRs are welcome!
Please keep submissions defensive and educational.

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE).

## âš–ï¸ Ethical Use

This project is intended **for defensive research and educational purposes only**.  
It does **not** contain real jailbreak payloads or exploits â€” only simulated examples for testing.  

Please **do not use this project to build or share offensive tools**.  
The goal of Inj3ctStop is to **understand, demonstrate, and improve defenses** against prompt injection attacks,  
so that the community can build safer LLM applications.